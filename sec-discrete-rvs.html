<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US" dir="ltr">
<head xmlns:og="http://ogp.me/ns#" xmlns:book="https://ogp.me/ns/book#">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Discrete random variables</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta property="og:type" content="book">
<meta property="book:title" content="Math 487 at UNL">
<meta property="book:author" content="Zach Norwood">
<script>
var runestoneMathReady = new Promise((resolve) => window.rsMathReady = resolve);
window.MathJax = {
  "tex": {
    "inlineMath": [
      [
        "\\(",
        "\\)"
      ]
    ],
    "tags": "none",
    "tagSide": "right",
    "tagIndent": ".8em",
    "packages": {
      "[+]": [
        "base",
        "extpfeil",
        "ams",
        "amscd",
        "color",
        "newcommand",
        "knowl"
      ]
    }
  },
  "options": {
    "ignoreHtmlClass": "tex2jax_ignore|ignore-math",
    "processHtmlClass": "process-math"
  },
  "chtml": {
    "scale": 0.98,
    "mtextInheritFont": true
  },
  "loader": {
    "load": [
      "input/asciimath",
      "[tex]/extpfeil",
      "[tex]/amscd",
      "[tex]/color",
      "[tex]/newcommand",
      "[pretext]/mathjaxknowl3.js"
    ],
    "paths": {
      "pretext": "https://pretextbook.org/js/lib"
    }
  },
  "startup": {
    pageReady() {
      return MathJax.startup.defaultPageReady().then(function () {
      console.log("in ready function");
      rsMathReady();
      }
    )}
  }
};
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lunr.js/2.3.9/lunr.min.js" integrity="sha512-4xUl/d6D6THrAnXAwGajXkoWaeMNwEKK4iNfq5DotEbLPAfk6FSxSP3ydNxqDgCw1c/0Z1Jg6L8h2j+++9BZmg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><script src="lunr-pretext-search-index.js" async=""></script><script src="https://pretextbook.org/js/0.33/pretext_search.js"></script><link href="https://pretextbook.org/css/0.83/pretext_search.css" rel="stylesheet" type="text/css">
<script>js_version = 0.33</script><script src="https://pretextbook.org/js/lib/jquery.min.js"></script><script src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script src="https://pretextbook.org/js/0.33/pretext.js"></script><script>miniversion=0.1</script><script src="https://pretextbook.org/js/0.33/pretext_add_on.js?x=1"></script><script src="https://pretextbook.org/js/0.33/user_preferences.js"></script><script src="https://pretextbook.org/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script>sagecellEvalName='Evaluate (Sage)';
</script><link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&amp;family=Noto+Serif:ital,wght@0,400;0,700;1,400;1,700&amp;family=Tinos:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap" rel="stylesheet">
<link href="https://fonts.cdnfonts.com/css/dejavu-serif" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Roboto+Serif:opsz,wdth,wght@8..144,50..150,100..900&amp;display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Open+Sans:wdth,wght@75..100,300..800&amp;display=swap" rel="stylesheet">
<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200">
<link href="https://pretextbook.org/css/0.83/pretext.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.83/pretext_add_on.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.83/shell_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.83/banner_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.83/navbar_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.83/toc_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.83/knowls_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.83/style_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.83/colors_blue_red.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.83/setcolors.css" rel="stylesheet" type="text/css">
</head>
<body class="pretext book ignore-math">
<a class="assistive" href="#ptx-content">Skip to main content</a><header id="ptx-masthead" class="ptx-masthead"><div class="ptx-banner">
<a id="logo-link" class="logo-link" target="_blank" href=""><img src="external/dice.png" alt="Logo image"></a><div class="title-container">
<h1 class="heading"><a href="math-487-notes.html"><span class="title">Math 487 at UNL:</span> <span class="subtitle">Fall 2023 lecture notes</span></a></h1>
<p class="byline">Zach Norwood</p>
</div>
</div></header><nav id="ptx-navbar" class="ptx-navbar navbar"><button class="toc-toggle button" title="Contents"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5d2;</span><span class="name">Contents</span></button><div class="searchbox">
<div class="searchwidget"><button id="searchbutton" class="searchbutton button" type="button" title="Search book"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe8b6;</span><span class="name">Search Book</span></button></div>
<div id="searchresultsplaceholder" class="searchresultsplaceholder" style="display: none">
<div class="search-results-controls">
<input aria-label="Search term" id="ptxsearch" class="ptxsearch" type="text" name="terms" placeholder="Search term"><button title="Close search" id="closesearchresults" class="closesearchresults"><span class="material-symbols-outlined">close</span></button>
</div>
<h2 class="search-results-heading">Search Results: </h2>
<div id="searchempty" class="searchempty"><span>No results.</span></div>
<ol id="searchresults" class="searchresults"></ol>
</div>
</div>
<span class="nav-other-controls"></span><span class="treebuttons"><a class="previous-button button" href="sec-random-variables-theory.html" title="Previous"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5cb;</span><span class="name">Prev</span></a><a class="up-button button" href="ch-random-variables.html" title="Up"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5ce;</span><span class="name">Up</span></a><a class="next-button button" href="sec-cts-rvs.html" title="Next"><span class="name">Next</span><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5cc;</span></a></span></nav><div id="latex-macros" class="hidden-content process-math" style="display:none"><span class="process-math">\(\newcommand{\R}{\mathbb R}
\newcommand{\N}{\mathbb N}
\newcommand {\abs} [1] {\left| #1 \right|}
\newcommand {\Bernoulli}{\operatorname{Bernoulli}}
\newcommand {\binomial}{\operatorname{binomial}}
\newcommand {\geometric}{\operatorname{geometric}}
\newcommand {\NegBinom} {\operatorname{NegBinom}}
\newcommand {\Poisson} {\operatorname{Poisson}}
\newcommand {\gauss} {\mathcal{N}}
\newcommand {\var} {\operatorname{var}}
\newcommand {\cov} {\operatorname{cov}}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\definecolor{fillinmathshade}{gray}{0.9}
\newcommand{\fillinmath}[1]{\mathchoice{\colorbox{fillinmathshade}{$\displaystyle     \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\textstyle        \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptstyle      \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptscriptstyle\phantom{\,#1\,}$}}}
\)</span></div>
<div class="ptx-page">
<div id="ptx-sidebar" class="ptx-sidebar"><nav id="ptx-toc" class="ptx-toc depth2"><ul class="structural contains-active toc-item-list">
<li class="toc-item toc-frontmatter"><div class="toc-title-box"><a href="frontmatter.html" class="internal"><span class="title">Front Matter</span></a></div></li>
<li class="toc-item toc-chapter">
<div class="toc-title-box"><a href="prob-spaces.html" class="internal"><span class="codenumber">1</span> <span class="title">Probability spaces</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-section"><div class="toc-title-box"><a href="prob-axioms.html" class="internal"><span class="codenumber">1.1</span> <span class="title">The Axioms of Probability</span></a></div></li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="prob-spaces-3.html" class="internal"><span class="codenumber">1.2</span> <span class="title">Conditional probability and independence</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="prob-spaces-3.html#subsec-conditional-prob" class="internal"><span class="codenumber">1.2.1</span> <span class="title">Conditional Probability</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="prob-spaces-3.html#subsec-independence" class="internal"><span class="codenumber">1.2.2</span> <span class="title">Independence</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-section"><div class="toc-title-box"><a href="sec-more-examples.html" class="internal"><span class="codenumber">1.3</span> <span class="title">More Examples</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-chapter contains-active">
<div class="toc-title-box"><a href="ch-random-variables.html" class="internal"><span class="codenumber">2</span> <span class="title">Random variables</span></a></div>
<ul class="structural toc-item-list contains-active">
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-random-variables-theory.html" class="internal"><span class="codenumber">2.1</span> <span class="title">Random variables: definition and examples</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-random-variables-theory.html#subsec-random-variables" class="internal"><span class="codenumber">2.1.1</span> <span class="title">Random variables: the basics</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-random-variables-theory.html#subsec-discrete-rvs" class="internal"><span class="codenumber">2.1.2</span> <span class="title">Discrete random variables</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-random-variables-theory.html#subsec-cts-rvs" class="internal"><span class="codenumber">2.1.3</span> <span class="title">Continuous random variables</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-section active">
<div class="toc-title-box"><a href="sec-discrete-rvs.html" class="internal"><span class="codenumber">2.2</span> <span class="title">Discrete random variables</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-discrete-rvs.html#subsec-expectation-discrete" class="internal"><span class="codenumber">2.2.1</span> <span class="title">The expectation of a discrete random variable</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-discrete-rvs.html#subsec-poisson" class="internal"><span class="codenumber">2.2.2</span> <span class="title">The Poisson Distribution</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-discrete-rvs.html#subsec-discrete-more-examples" class="internal"><span class="codenumber">2.2.3</span> <span class="title">More examples of discrete distributions</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-discrete-rvs.html#subsec-variance-correlation" class="internal"><span class="codenumber">2.2.4</span> <span class="title">Correlation and variance</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-section">
<div class="toc-title-box"><a href="sec-cts-rvs.html" class="internal"><span class="codenumber">2.3</span> <span class="title">Continuous random variables</span></a></div>
<ul class="structural toc-item-list">
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-cts-rvs.html#subsec-exponential" class="internal"><span class="codenumber">2.3.1</span> <span class="title">The exponential distribution</span></a></div></li>
<li class="toc-item toc-subsection"><div class="toc-title-box"><a href="sec-cts-rvs.html#subsec-gaussian" class="internal"><span class="codenumber">2.3.2</span> <span class="title">The Gaussian distribution</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-section"><div class="toc-title-box"><a href="sec-some-inequalities.html" class="internal"><span class="codenumber">2.4</span> <span class="title">Some Inequalities</span></a></div></li>
</ul>
</li>
<li class="toc-item toc-chapter">
<div class="toc-title-box"><a href="ch-joint-distributions.html" class="internal"><span class="codenumber">3</span> <span class="title">Jointly distributed random variables</span></a></div>
<ul class="structural toc-item-list"><li class="toc-item toc-section"><div class="toc-title-box"><a href="ch-joint-distributions-2.html" class="internal"><span class="codenumber">3.1</span> <span class="title">Definitions and examples</span></a></div></li></ul>
</li>
<li class="toc-item toc-backmatter"><div class="toc-title-box"><a href="backmatter.html" class="internal"><span class="title">Backmatter</span></a></div></li>
</ul></nav></div>
<main class="ptx-main"><div id="ptx-content" class="ptx-content"><section class="section" id="sec-discrete-rvs"><h2 class="heading hide-type">
<span class="type">Section</span><span class="space"> </span><span class="codenumber">2.2</span><span class="space"> </span><span class="title">Discrete random variables</span>
</h2>
<section class="introduction" id="sec-discrete-rvs-2"><article class="example example-like" id="sec-discrete-rvs-2-1"><h3 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">2.2.1</span><span class="period">.</span>
</h3>
<div class="para" id="sec-discrete-rvs-2-1-1-1">We perform <span class="process-math">\(n\)</span> independent trials, each with a fixed probability <span class="process-math">\(p\)</span> of success. The total number of successes at the end is a <dfn class="terminology">binomial random variable</dfn> with parameters <span class="process-math">\(n\)</span> and <span class="process-math">\(p\text{;}\)</span> we write <span class="process-math">\(X \sim \binomial(n,p)\text{.}\)</span> Find its probability mass function.</div>
<div class="solutions"><details id="sec-discrete-rvs-2-1-2" class="solution solution-like born-hidden-knowl"><summary><span class="type">Solution</span><span class="period">.</span></summary><div class="solution solution-like">
<div class="para logical" id="sec-discrete-rvs-2-1-2-1">
<div class="para">Fix <span class="process-math">\(k \le n\text{.}\)</span> There are <span class="process-math">\(\binom nk\)</span>-many ways of choosing which of the <span class="process-math">\(k\)</span> trials will end in success. Then the probability that these <span class="process-math">\(k\)</span> trials succeed and the other <span class="process-math">\(n-k\)</span> fail is (by independence) <span class="process-math">\(p^k (1-p)^{n-k}\text{.}\)</span> Multiplying these gives the value <span class="process-math">\(p(k)\)</span> of the pmf at <span class="process-math">\(k\text{:}\)</span>
</div>
<div class="displaymath process-math">
\begin{equation*}
p(k) = \binom nk p^k (1-p)^{n-k}
\end{equation*}
</div>
<div class="para">You can view a graph of this pmf at <a class="external" href="https://www.geogebra.org/calculator/ykvdawbt" target="_blank">this GeoGebra link</a><a class="xref" id="sec-discrete-rvs-2-1-2-1-11" href="" data-knowl="./knowl/fn/sec-discrete-rvs-2-1-2-1-11.html" title="Footnote 2.2.1"><sup> 1 </sup></a>.</div>
</div> <div class="para" id="sec-discrete-rvs-2-1-2-2">Later we will make precise the guess that the “center of mass” of this pmf occurs at <span class="process-math">\(p n\text{.}\)</span>
</div>
</div></details></div></article> <details id="sec-discrete-rvs-2-2" class="exercise exercise-like born-hidden-knowl"><summary></summary><article class="exercise exercise-like"></article></details> <article class="definition definition-like" id="def-indep-rv"><h3 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">2.2.2</span><span class="period">.</span>
</h3>
<div class="para" id="def-indep-rv-1-1">Two random variables <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\)</span> on the same probability space are <dfn class="terminology">independent</dfn> if for all <span class="process-math">\(x,y\in\R\)</span> the events <span class="process-math">\(\{X \le x\}\)</span> and <span class="process-math">\(\{Y \le y\}\)</span> are independent.</div> <div class="para" id="def-indep-rv-1-2">If <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\)</span> are discrete, then it is equivalent to require that for all <span class="process-math">\(x,y\in\R\)</span> the events <span class="process-math">\(\{X = x\}\)</span> and <span class="process-math">\(\{Y = y\}\)</span> are independent.</div></article> <div class="para logical" id="sec-discrete-rvs-2-4">
<div class="para">Then a random variable is <span class="process-math">\(\binomial(n,p)\)</span> if and only if it is a sum of <span class="process-math">\(n\)</span> independent <span class="process-math">\(\Bernoulli(p)\)</span> random variables:</div>
<div class="displaymath process-math">
\begin{equation*}
Y = X_1 + \cdots + X_n
\end{equation*}
</div>
</div> <article class="example example-like" id="sec-discrete-rvs-2-5"><h3 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">2.2.3</span><span class="period">.</span>
</h3>
<div class="para" id="sec-discrete-rvs-2-5-1-1">Independent trials are performed, each with a fixed probability <span class="process-math">\(p\)</span> of success. The number of trials required to see the first success is a <dfn class="terminology">geometric</dfn> random variable with parameter <span class="process-math">\(p\text{,}\)</span> and if it’s called <span class="process-math">\(X\)</span> then we write <span class="process-math">\(X \sim \geometric(p)\text{.}\)</span> Compute the pmf of a <span class="process-math">\(\geometric(p)\)</span> random variable.</div>
<div class="solutions"><details id="sec-discrete-rvs-2-5-2" class="solution solution-like born-hidden-knowl"><summary><span class="type">Solution</span><span class="period">.</span></summary><div class="solution solution-like"><div class="para logical" id="sec-discrete-rvs-2-5-2-1">
<div class="para">The possible values of <span class="process-math">\(X\)</span> are <span class="process-math">\(1,2,3,\dots\text{.}\)</span> Fix a positive integer <span class="process-math">\(k\text{.}\)</span> The probability that the first <span class="process-math">\(k-1\)</span> trials fail and the <span class="process-math">\(k\)</span>th succeeds is</div>
<div class="displaymath process-math">
\begin{equation*}
f(k) = (1-p)^{k-1} p
\end{equation*}
</div>
</div></div></details></div></article> <details id="sec-discrete-rvs-2-6" class="exercise exercise-like born-hidden-knowl"><summary></summary><article class="exercise exercise-like"></article></details></section><section class="subsection" id="subsec-expectation-discrete"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">2.2.1</span><span class="space"> </span><span class="title">The expectation of a discrete random variable</span>
</h3>
<article class="definition definition-like" id="def-expectation-discrete"><h4 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">2.2.4</span><span class="period">.</span>
</h4>
<div class="para logical" id="def-expectation-discrete-1-1">
<div class="para">The <dfn class="terminology">expected value</dfn>, <dfn class="terminology">expectation</dfn>, or <dfn class="terminology">mean</dfn> of a discrete random variable <span class="process-math">\(X\)</span> with pmf <span class="process-math">\(f\)</span> is defined as follows.</div>
<div class="displaymath process-math">
\begin{equation*}
E(X) = \sum_{x} xf(x) = \sum_i x_i p_i,
\end{equation*}
</div>
<div class="para">whenever this sum converges absolutely. (Otherwise the expected value does not exist.)</div>
</div></article><details id="subsec-expectation-discrete-3" class="exercise exercise-like born-hidden-knowl"><summary><h4 class="heading">
<span class="type">Checkpoint</span><span class="space"> </span><span class="codenumber">2.2.5</span><span class="period">.</span>
</h4></summary><article class="exercise exercise-like"><div class="para" id="subsec-expectation-discrete-3-1">Verify that this agrees with the gradeschool notion of mean in the case <span class="process-math">\(p_1 = p_2 = \cdots = p_n = \tfrac1n\text{.}\)</span>
</div></article></details><article class="example example-like" id="subsec-expectation-discrete-4"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">2.2.6</span><span class="period">.</span>
</h4>
<div class="para logical" id="subsec-expectation-discrete-4-1">
<div class="para">A <span class="process-math">\(\Bernoulli(p)\)</span> random variable has mean <span class="process-math">\(p\text{:}\)</span>
</div>
<div class="displaymath process-math">
\begin{equation*}
\sum_x x f(x) = 0\cdot f(0) + 1 \cdot f(1) = 0(1-p) + p = p.
\end{equation*}
</div>
</div> <div class="para" id="subsec-expectation-discrete-4-2">In particular, <span class="process-math">\(E(1_A) = P(A)\text{.}\)</span>
</div></article><article class="example example-like" id="subsec-expectation-discrete-5"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">2.2.7</span><span class="period">.</span>
</h4>
<div class="para logical" id="subsec-expectation-discrete-5-1">
<div class="para">In order to compute the expectation of <span class="process-math">\(Y \sim \binomial(n,p)\text{,}\)</span> we must make sense of the following quantity.</div>
<div class="displaymath process-math">
\begin{equation*}
E(Y) = \sum_{k=0}^n k \binom nk p^k (1-p)^{n-k}
\end{equation*}
</div>
<div class="para">After some thought, we might notice that this is the following function of <span class="process-math">\(x\)</span> and <span class="process-math">\(y\)</span> evaluated at <span class="process-math">\(x = p\text{,}\)</span> <span class="process-math">\(y = 1-p\text{.}\)</span>
</div>
<div class="displaymath process-math" id="subsec-expectation-discrete-5-1-7">
\begin{align*}
\sum_{k=0}^n k \binom nk x^k y^{n-k}
\amp = x \frac{d}{dx} \sum_{k=0}^n \binom nk x^k y^{n-k} \\
\amp = x \frac{d}{dx} (x+y)^{n} \\
\amp = x n (x+y)^{n-1}  
\end{align*}
</div>
<div class="para">(We have used the Binomial Theorem.) Now, plugging in <span class="process-math">\(x = p\)</span> and <span class="process-math">\(y = 1-p\)</span> gives</div>
<div class="displaymath process-math">
\begin{equation*}
pn(p + 1-p)^{n-1} = pn.
\end{equation*}
</div>
<div class="para">But this is much easier using the linearity of expectation!</div>
</div></article><article class="theorem theorem-like" id="thm-linearity-expectation-discrete"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">2.2.8</span><span class="period">.</span>
</h4>
<div class="para logical" id="thm-linearity-expectation-discrete-1-1">
<div class="para">The expectation operator <span class="process-math">\(E\)</span> is linear; that is,</div>
<div class="displaymath process-math" id="thm-linearity-expectation-discrete-1-1-2">
\begin{align*}
E(aX) \amp = a E(X) \\
\text{and} \quad E(X+Y) \amp = E(X) + E(Y) 
\end{align*}
</div>
</div></article><details id="thm-linearity-expectation-discrete-2" class="hiddenproof born-hidden-knowl"><summary><h4 class="heading"><span class="type">Proof<span class="period">.</span></span></h4></summary><article class="hiddenproof"><div class="para" id="thm-linearity-expectation-discrete-2-1">It is useful to notice that, for fixed <span class="process-math">\(x\in\R\text{,}\)</span> <span class="process-math">\(\{X = x\} = \bigcup_y \{X = x \text{ and } Y = y\}\text{,}\)</span> and that this is a disjoint union. Thus, by the additivity of the probability measure, <span class="process-math">\(P(X = x) = \sum_y P(X = x \text{ and } Y = y)\text{.}\)</span> We will use this fact and a similar one with the roles of <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\)</span> interchanged in what follows.</div> <div class="para logical" id="thm-linearity-expectation-discrete-2-2">
<div class="displaymath process-math" id="thm-linearity-expectation-discrete-2-2-1">
\begin{align*}
E(X+Y) \amp = \sum_z zP(X+Y = z)\\
\amp = \sum_x \sum_y (x+y) P(X = x \text{ and } Y = y) \\
\amp = \sum_x x \left[ \sum_y P(X = x \text{ and } Y = y) \right] 
+ \sum_y y \left[ \sum_x P(X = x \text{ and } Y = y) \right]\\
\amp = \sum_x x P(X = x) + \sum_y y P(Y = y)\\
\amp = E(X) + E(Y)
\end{align*}
</div>
<div class="para">The proof that <span class="process-math">\(E(aX) = aE(X)\)</span> is easier and is left as an exercise.</div>
</div></article></details><article class="example example-like" id="subsec-expectation-discrete-7"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">2.2.9</span><span class="period">.</span>
</h4>
<div class="para logical" id="subsec-expectation-discrete-7-1">
<div class="para">Now it is much easier to compute the expectation of a binomial random variable: if <span class="process-math">\(Y \sim \binomial(n,p)\)</span> then there are independent <span class="process-math">\(\Bernoulli(p)\)</span> random variables <span class="process-math">\(X_1,\dots,X_n\)</span> of which <span class="process-math">\(Y\)</span> is the sum, and now we can use the linearity of expectation:</div>
<div class="displaymath process-math">
\begin{equation*}
E(Y) = E(X_1) + \cdots + E(X_n) = np.
\end{equation*}
</div>
</div></article><article class="lemma theorem-like" id="lem-lotus"><h4 class="heading">
<span class="type">Lemma</span><span class="space"> </span><span class="codenumber">2.2.10</span><span class="period">.</span>
</h4>
<div class="para logical" id="lem-lotus-1-1">
<div class="para">Suppose that <span class="process-math">\(X\)</span> is a discrete random variable with pmf <span class="process-math">\(f\)</span> and that <span class="process-math">\(g\colon\R\to\R\)</span> is any function. Then the expectation of the random variable <span class="process-math">\(g(X)\)</span> can be computed as follows.</div>
<div class="displaymath process-math">
\begin{equation*}
E(g(X)) = \sum_x g(x) f(x).
\end{equation*}
</div>
</div></article><details id="lem-lotus-2" class="hiddenproof born-hidden-knowl"><summary><h4 class="heading"><span class="type">Proof<span class="period">.</span></span></h4></summary><article class="hiddenproof"><div class="para" id="lem-lotus-2-1">Exercise.</div></article></details><article class="definition definition-like" id="def-moments"><h4 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">2.2.11</span><span class="period">.</span>
</h4>
<div class="para" id="def-moments-1-1">Let <span class="process-math">\(X\)</span> be a random variable. For <span class="process-math">\(k\in\N\)</span> we define the <span class="process-math">\(k\)</span>th <dfn class="terminology">moment</dfn> of <span class="process-math">\(X\text{,}\)</span> denoted <span class="process-math">\(m_k(X)\text{,}\)</span> to be <span class="process-math">\(E(X^k)\text{.}\)</span> The <span class="process-math">\(k\)</span>th <dfn class="terminology">central moment</dfn> of <span class="process-math">\(X\text{,}\)</span> denoted <span class="process-math">\(\sigma_k(X)\text{,}\)</span> to be <span class="process-math">\(E((X- E(X))^k)\text{.}\)</span>
</div> <div class="para" id="def-moments-1-2">Of particular note is <span class="process-math">\(\sigma_2 = E((X - E(X))^2) = \var(X)\text{,}\)</span> the <dfn class="terminology">variance</dfn> of <span class="process-math">\(X\text{.}\)</span>
</div></article><div class="para" id="subsec-expectation-discrete-10">The variance of <span class="process-math">\(X\)</span> measures the tendency of <span class="process-math">\(X\)</span> to deviate from its mean.</div>
<details id="subsec-expectation-discrete-11" class="exercise exercise-like born-hidden-knowl"><summary><h4 class="heading">
<span class="type">Checkpoint</span><span class="space"> </span><span class="codenumber">2.2.12</span><span class="period">.</span>
</h4></summary><article class="exercise exercise-like"><div class="para" id="subsec-expectation-discrete-11-1">Show that <span class="process-math">\(\var(X) = E(X^2) - E(X)^2\text{.}\)</span>
</div></article></details><article class="example example-like" id="subsec-expectation-discrete-12"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">2.2.13</span><span class="period">.</span>
</h4>
<div class="para" id="subsec-expectation-discrete-12-1-1">Compute the variance of a <span class="process-math">\(\Bernoulli(p)\)</span> random variable.</div>
<div class="solutions"><details id="subsec-expectation-discrete-12-2" class="solution solution-like born-hidden-knowl"><summary><span class="type">Solution</span><span class="period">.</span></summary><div class="solution solution-like"><div class="para logical" id="subsec-expectation-discrete-12-2-1">
<div class="para">We see that</div>
<div class="displaymath process-math">
\begin{equation*}
E(X^2) = \sum_x x^2 f(x) = 0^2 (1-p) + 1^2 p = p,
\end{equation*}
</div>
<div class="para">so the variance is given by</div>
<div class="displaymath process-math">
\begin{equation*}
\var(X) = E(X^2) - E(X)^2 = p - p^2 = p(1-p).
\end{equation*}
</div>
</div></div></details></div></article><article class="example example-like" id="subsec-expectation-discrete-13"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">2.2.14</span><span class="period">.</span><span class="space"> </span><span class="title">Matching, revisited.</span>
</h4>
<div class="para logical" id="subsec-expectation-discrete-13-2-1">
<div class="para">Recall <a href="" class="xref" data-knowl="./knowl/xref/ex-matching-hats.html" title="Example 1.3.3: Matching hats">Example 1.3.3</a>, in which we determined that, when <span class="process-math">\(n\)</span> people’s hats are randomly shuffled, the probability that at least one person gets their hat back is</div>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/xref/ex-matching-hats.html">
\begin{equation*}
1 - \frac{1}{2!} + \frac{1}{3!} - \cdots + \frac{(-1)^{n+1}}{n!},
\end{equation*}
</div>
<div class="para">a quantity that tends as <span class="process-math">\(n\to\infty\)</span> to <span class="process-math">\(1 - \tfrac1e\text{.}\)</span>
</div>
</div> <div class="para" id="subsec-expectation-discrete-13-2-2">Now, letting <span class="process-math">\(X\)</span> be the number of people who receive their own hat, find <span class="process-math">\(E(X)\)</span> and <span class="process-math">\(\var(X)\text{.}\)</span>
</div>
<div class="solutions"><details id="subsec-expectation-discrete-13-3" class="solution solution-like born-hidden-knowl"><summary><span class="type">Solution</span><span class="period">.</span></summary><div class="solution solution-like">
<div class="para logical" id="subsec-expectation-discrete-13-3-1">
<div class="para">Consider, for <span class="process-math">\(k=1,\dots,n\text{,}\)</span> the following indicator random variable.</div>
<div class="displaymath process-math">
\begin{equation*}
I_k = \begin {cases}
1 \amp \text{ if the } k \text{th person gets his hat back} \\
0 \amp \text{ otherwise}
\end {cases}
\end{equation*}
</div>
<div class="para">Notice that <span class="process-math">\(E(I_k)\)</span> equals the probability that the <span class="process-math">\(k\)</span>th person gets their hat back, which as we discovered in the previous example is <span class="process-math">\(\tfrac1n\text{.}\)</span> Now we use the linearity of expectation:</div>
<div class="displaymath process-math">
\begin{equation*}
E(X) 
= E\left( \sum_{k=1}^n I_k \right)
= \sum_{k=1}^n E(X_k) = \sum_{k=1}^n \frac1n = 1.
\end{equation*}
</div>
</div> <div class="para logical" id="subsec-expectation-discrete-13-3-2">
<div class="displaymath process-math" id="subsec-expectation-discrete-13-3-2-1">
\begin{align*}
E(X^2) \amp = E \left[ \left( \sum_{k=1}^n I_k \right)^2 \right]\\
\amp = E \left[ \sum_{k=1}^n I_k^2 + \sum_{j\ne k} I_j I_k \right]\\
\amp = \sum_{k=1}^n E[I_k^2] + \sum_{j \ne k} E[I_j I_k]\\
\amp = \sum_{k=1}^n \frac{1}{n} + \sum_{j\ne k} \frac{1}{n(n-1)} \\
\amp = 1 + n(n-1)\frac{1}{n(n-1)}\\
\amp = 2
\end{align*}
</div>
<div class="para">For that we needed to compute <span class="process-math">\(E[I_j I_k]\text{:}\)</span>
</div>
<div class="displaymath process-math" id="subsec-expectation-discrete-13-3-2-3">
\begin{align*}
E(I_j I_k) \amp = 1P(\text{both } j \text{ and } k \text{ get their hat back}) + 0P(\text{at least one doesn't}) \\
\amp = 1\cdot \frac{1}{n(n-1)} + 0 
\end{align*}
</div>
<div class="para">Now we can conclude that</div>
<div class="displaymath process-math">
\begin{equation*}
\var(X) = E(X^2) - E(X)^2 = 2 - 1 = 1.
\end{equation*}
</div>
</div>
</div></details></div></article><article class="example example-like" id="subsec-expectation-discrete-14"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">2.2.15</span><span class="period">.</span><span class="space"> </span><span class="title">Coupon Collector.</span>
</h4>
<div class="para logical" id="subsec-expectation-discrete-14-2-1">
<div class="para">A fast-food restaurant offers kiddie meals that each include a toy. There are <span class="process-math">\(N\)</span> different toys available, and toys are equally likely to be in a given meal.</div>
<ol class="lower-alpha">
<li id="subsec-expectation-discrete-14-2-1-2-1"><div class="para" id="subsec-expectation-discrete-14-2-1-2-1-1">Find the expected number of meals needed to collect all the toys.</div></li>
<li id="subsec-expectation-discrete-14-2-1-2-2"><div class="para" id="subsec-expectation-discrete-14-2-1-2-2-1">Find the expected number of different toys collected if <span class="process-math">\(M\)</span> kiddie meals are purchased.</div></li>
</ol>
</div>
<div class="solutions"><details id="subsec-expectation-discrete-14-3" class="solution solution-like born-hidden-knowl"><summary><span class="type">Solution</span><span class="period">.</span></summary><div class="solution solution-like">
<div class="para" id="subsec-expectation-discrete-14-3-1">Let <span class="process-math">\(X\)</span> be the number of meals before all <span class="process-math">\(N\)</span> toys are collected. Then <span class="process-math">\(X = X_0 + X_1 + \cdots + X_{N-1}\text{,}\)</span> where <span class="process-math">\(X_k\)</span> is the number of additional meals after <span class="process-math">\(k\)</span> different toys have collected to get another toy. By linearity of expectation, <span class="process-math">\(E(X) = \sum_{k=0}^{N-1} E(X_k)\text{.}\)</span> Notice that <span class="process-math">\(X_k \sim \geometric\left(\frac{N-k}{N}\right)\text{,}\)</span> since after <span class="process-math">\(k\)</span> different toys have been collected, the chance of any particular meal giving us a new toy is <span class="process-math">\(\frac{N-k}{N}\text{.}\)</span>
</div> <div class="para logical" id="subsec-expectation-discrete-14-3-2">
<div class="para">Recalling that a <span class="process-math">\(\geometric(p)\)</span> random variable has expectation <span class="process-math">\(\tfrac1p\text{,}\)</span> we are now in position to compute the expectation of <span class="process-math">\(X\text{.}\)</span>
</div>
<div class="displaymath process-math" id="subsec-expectation-discrete-14-3-2-4">
\begin{align*}
E(X) \amp = \sum_{k=0}^{N-1} E(X_k) \\
\amp = 1 + \frac{N}{N-1} + \frac{N}{N-2} + \cdots + N\\
\amp = N\left( \frac{1}{N} + \frac{1}{N-1} + \cdots + 1\right) \\
\amp = N H(N) ,
\end{align*}
</div>
<div class="para">where <span class="process-math">\(H(N)\)</span> is the <span class="process-math">\(N\)</span>th <dfn class="terminology">harmonic number</dfn>. This quantity for <span class="process-math">\(N=6\)</span> is approximately <span class="process-math">\(15\)</span> and for <span class="process-math">\(N = 8\)</span> is approximately <span class="process-math">\(22\text{.}\)</span>
</div>
</div> <div class="para logical" id="subsec-expectation-discrete-14-3-3">
<div class="para">For part (b), let <span class="process-math">\(X\)</span> be the number of different toys collected if <span class="process-math">\(M\)</span> kiddie meals are purchased, and let <span class="process-math">\(Y = N - X\)</span> be the number of toys missing from the collection after <span class="process-math">\(M\)</span> meals. Then <span class="process-math">\(Y\)</span> can be written as a sum</div>
<div class="displaymath process-math">
\begin{equation*}
Y = Y_1 + \cdots + Y_N,
\end{equation*}
</div>
<div class="para">where</div>
<div class="displaymath process-math">
\begin{equation*}
Y_k = \begin {cases} 0 \amp \text{ if the } k \text{th toy is in the collection} \\
1 \amp \text{ if it's missing}
\end {cases}
\end{equation*}
</div>
<div class="para">Note that <span class="process-math">\(E(Y_k) = 1p+0(1-p) = p\text{,}\)</span> where <span class="process-math">\(p\)</span> is the probability that we didn’t get the <span class="process-math">\(k\)</span>th toy in <span class="process-math">\(M\)</span> meals, which is <span class="process-math">\(\left( \frac{N-1}{N}\right)^M\text{.}\)</span> Now <span class="process-math">\(E(Y) = \sum_{k=1}^N E(Y_k) = N\left(1-\frac{1}{N}\right)^M\text{.}\)</span> And we can compute the expected value of <span class="process-math">\(X\text{:}\)</span>
</div>
<div class="displaymath process-math">
\begin{equation*}
E(X) = N - E(Y) = N - N\left(1 - \frac{1}{N}\right)^M
\end{equation*}
</div>
<div class="para">Notice that this quantity approaches <span class="process-math">\(N\)</span> as <span class="process-math">\(M\to\infty\text{,}\)</span> as expected.</div>
</div>
</div></details></div></article></section><section class="subsection" id="subsec-poisson"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">2.2.2</span><span class="space"> </span><span class="title">The Poisson Distribution</span>
</h3>
<div class="para logical" id="subsec-poisson-2">
<div class="para">Consider <span class="process-math">\(X \sim \binomial(n,p)\)</span> and let <span class="process-math">\(n\to\infty\)</span> and <span class="process-math">\(p\to 0\)</span> while fixing <span class="process-math">\(\lambda = np\text{:}\)</span>
</div>
<div class="displaymath process-math" id="subsec-poisson-2-5">
\begin{align*}
\binom nk p^k (1-p)^{n-k} \amp = \binom nk \left( \frac{\lambda}{n} \right)^k \left( 1 - \frac{\lambda}{n}\right)^{n-k} \\
\amp = \frac{n(n-1)(n-2)\cdots(n-k+1)}{k!} \cdot \frac{\lambda^k}{n^k}\left( 1 - \frac{\lambda}{n}\right)^n \left( 1 - \frac{\lambda}{n}\right)^{-k}\\
\amp = \frac{n(n-1)(n-2)\cdots(n-k+1)}{n^k} \cdot \frac{\lambda^k}{k!}\left( 1 - \frac{\lambda}{n}\right)^n \left( 1 - \frac{\lambda}{n}\right)^{-k}\\
\amp \underset{n\to\infty}{\longrightarrow} 1 \cdot \frac{\lambda^k}{k!} \cdot e^{-\lambda} \cdot 1
= \frac{\lambda^k}{k!} e^{-\lambda} 
\end{align*}
</div>
</div>
<article class="definition definition-like" id="def-poisson"><h4 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">2.2.16</span><span class="period">.</span>
</h4>
<div class="para logical" id="def-poisson-1-1">
<div class="para">A random variable is said to follow a <dfn class="terminology">Poisson</dfn> distribution with parameter <span class="process-math">\(\lambda\in (0,\infty)\)</span> if its pmf <span class="process-math">\(f\)</span> is given by</div>
<div class="displaymath process-math">
\begin{equation*}
f(k) = \frac{\lambda^k}{k!} e^{-\lambda}
\end{equation*}
</div>
</div></article><div class="para" id="subsec-poisson-4">A <span class="process-math">\(\Poisson(\lambda)\)</span> random variable is a good model for a rare event that occurs on average <span class="process-math">\(\lambda\)</span> times per unit time.</div>
<details id="subsec-poisson-5" class="exercise exercise-like born-hidden-knowl"><summary><h4 class="heading">
<span class="type">Checkpoint</span><span class="space"> </span><span class="codenumber">2.2.17</span><span class="period">.</span>
</h4></summary><article class="exercise exercise-like"><div class="para" id="subsec-poisson-5-1">Verify that <span class="process-math">\(\sum_{k\ge 0} \frac{\lambda^k}{k!} e^{-\lambda} = 1\text{.}\)</span>
</div></article></details><article class="example example-like" id="subsec-poisson-6"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">2.2.18</span><span class="period">.</span>
</h4>
<div class="para" id="subsec-poisson-6-1-1">Compute the expected value of a <span class="process-math">\(\Poisson(\lambda)\)</span> random variable.</div>
<div class="solutions"><details id="subsec-poisson-6-2" class="solution solution-like born-hidden-knowl"><summary><span class="type">Solution</span><span class="period">.</span></summary><div class="solution solution-like"><div class="para logical" id="subsec-poisson-6-2-1"><div class="displaymath process-math" id="subsec-poisson-6-2-1-1">
\begin{align*}
E(X) \amp = \sum_{k\ge 0} k f(k) \\
\amp = \sum_{k\ge 0} k \frac{\lambda^k}{k!} e^{-\lambda} \\
\amp = \lambda e^{-\lambda} \sum_{k\ge 1} \frac{\lambda^{k-1}}{(k-1)!}\\
\amp = \lambda e^{-\lambda} \sum_{m\ge0} \frac{\lambda^m}{m!} \\
\amp = \lambda e^{-\lambda} e^{\lambda} \\
\amp = \lambda 
\end{align*}
</div></div></div></details></div></article><details id="subsec-poisson-7" class="exercise exercise-like born-hidden-knowl"><summary><h4 class="heading">
<span class="type">Checkpoint</span><span class="space"> </span><span class="codenumber">2.2.19</span><span class="period">.</span>
</h4></summary><article class="exercise exercise-like"><div class="para" id="subsec-poisson-7-1">Show that <span class="process-math">\(\var(X) = \lambda\)</span> too.</div></article></details><article class="example example-like" id="subsec-poisson-8"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">2.2.20</span><span class="period">.</span>
</h4>
<div class="para" id="subsec-poisson-8-1-1">Astronomers estimate that on average one large meteorite hits Earth every 100 years. Compute the probability that no large meteorite hits Earth in the next 100 years.</div>
<div class="solutions"><details id="subsec-poisson-8-2" class="solution solution-like born-hidden-knowl"><summary><span class="type">Solution</span><span class="period">.</span></summary><div class="solution solution-like"><div class="para" id="subsec-poisson-8-2-1">We model the number of meteorites coming in the next century by a <span class="process-math">\(\Poisson(1)\)</span> random variable. The probability that no meteorite comes in the next century is <span class="process-math">\(\frac{1e^{-1}}{0!} = \frac{1}{e} \approx 0.37\text{.}\)</span> This means there is a <span class="process-math">\(63\%\)</span> chance that at least one meteorite will hit in the next 100 years.</div></div></details></div></article><article class="example example-like" id="subsec-poisson-9"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">2.2.21</span><span class="period">.</span>
</h4>
<div class="para" id="subsec-poisson-9-1-1">Our observations indicate that on average <span class="process-math">\(1\)</span> gram of radioactive material discharges <span class="process-math">\(3.2\)</span> ɑ particles per second. Compute the probability that at most two ɑ particles will appear.</div>
<div class="solutions"><details id="subsec-poisson-9-2" class="solution solution-like born-hidden-knowl"><summary><span class="type">Solution</span><span class="period">.</span></summary><div class="solution solution-like"><div class="para logical" id="subsec-poisson-9-2-1">
<div class="displaymath process-math" id="subsec-poisson-9-2-1-1">
\begin{align*}
P(E) \amp = f(0) + f(1) + f(2) \\
\amp = e^{-3.2} + 3.2 e^{-3.2} + \frac{3.2^2}{2!}e^{-3.2} \\
\amp \approx 0.3799 
\end{align*}
</div>
<div class="para">So approximately a <span class="process-math">\(38\%\)</span> chance.</div>
</div></div></details></div></article><article class="example example-like" id="subsec-poisson-10"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">2.2.22</span><span class="period">.</span><span class="space"> </span><span class="title">Poisson Coin Flips.</span>
</h4>
<div class="para" id="subsec-poisson-10-2-1">Suppose that a coin with probability <span class="process-math">\(p\)</span> of showing heads is tossed <span class="process-math">\(N\)</span> times. Let <span class="process-math">\(X\)</span> be the number of heads and <span class="process-math">\(Y\)</span> the number of tails. Then <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\)</span> are certainly not independent; from the value of one we can compute the value of the other!</div> <div class="para" id="subsec-poisson-10-2-2">Nonetheless, show that if we toss the coin a <em class="emphasis">random</em> number <span class="process-math">\(N \sim \Poisson(\lambda)\)</span> of times, then <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\)</span> are independent!</div>
<div class="solutions"><details id="subsec-poisson-10-3" class="solution solution-like born-hidden-knowl"><summary><span class="type">Solution</span><span class="period">.</span></summary><div class="solution solution-like"><div class="para logical" id="subsec-poisson-10-3-1">
<div class="para">We must show for all <span class="process-math">\(k,l\)</span> that the events <span class="process-math">\(\{X = k\}\)</span> and <span class="process-math">\(\{Y = l\}\)</span> are independent, i.e., that</div>
<div class="displaymath process-math">
\begin{equation*}
P(X = k \text{ and } Y = l) = P(X = k) P(Y = l).
\end{equation*}
</div>
<div class="para">Start with the lefthand side:</div>
<div class="displaymath process-math" id="subsec-poisson-10-3-1-5">
\begin{align*}
P(X = k \text{ and } Y = l) \amp = P(X = k \text{ and } Y = l \mid N = k+l)P(N = k+l) \\
\amp = \binom{k+l}{l} p^k (1-p)^l \frac{\lambda^{k+l}}{(k+l)!} e^{-\lambda} \\
\amp = \frac{(k+l)!}{k! l!} p^k (1-p)^l \frac{\lambda^{k+l}}{(k+l)!} e^{-\lambda} \\
\amp = \frac{ (p\lambda)^k ((1-p)\lambda)^l}{k! l! } e^{-\lambda}
\end{align*}
</div>
<div class="para">Toward the righthand side, start by using the Law of Total Probability:</div>
<div class="displaymath process-math" id="subsec-poisson-10-3-1-6">
\begin{align*}
P(X = k) \amp = \sum_{n \ge k} P(X = k \mid N = n) P(N = n) \\
\amp = \sum_{n\ge k} \binom nk p^k (1-p)^{n-k} \frac{\lambda^n}{n!} e^{-\lambda} \\
\amp = \frac{p^k e^{-\lambda}}{k!} \lambda^k \sum_{n \ge k} \frac{(1-p)^{n-k}}{(n-k)!} \lambda^{n-k} \\
\amp = \frac{p^k e^{-\lambda}}{k!} \lambda^k \sum_{m \ge 0} \frac{(1-p)^m}{m!} \lambda^m \\
\amp = \frac{(\lambda p)^k e^{-\lambda p}}{k!} 
\end{align*}
</div>
<div class="para">Similarly,</div>
<div class="displaymath process-math">
\begin{equation*}
P(Y = l) = \frac{ (\lambda(1-p))^l e^{-\lambda(1-p)}}{l !}
\end{equation*}
</div>
<div class="para">Combining all this, we get:</div>
<div class="displaymath process-math">
\begin{equation*}
P(X = k)P(Y = l) = \frac{(\lambda p)^k (\lambda(1-p))^l e^{-\lambda}}{ k! l!} = P(X = k \text{ and } Y = l)
\end{equation*}
</div>
</div></div></details></div></article></section><section class="subsection" id="subsec-discrete-more-examples"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">2.2.3</span><span class="space"> </span><span class="title">More examples of discrete distributions</span>
</h3>
<article class="definition definition-like" id="def-neg-binom"><h4 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">2.2.23</span><span class="period">.</span>
</h4>
<div class="para logical" id="def-neg-binom-1-1">
<div class="para">Independent <span class="process-math">\(\Bernoulli(p)\)</span> trials are performed until a total of <span class="process-math">\(r\)</span> successes are achieved. Let <span class="process-math">\(X\)</span> be the number of trials required. Notice that the pmf of <span class="process-math">\(X\)</span> is given by the following formula.</div>
<div class="displaymath process-math">
\begin{equation*}
P(X = n) = \binom{n-1}{r-1} p^r (1-p)^{n-r}, \qquad n=r,r+1,\dots
\end{equation*}
</div>
<div class="para">(The <span class="process-math">\(n\)</span>th trial must be a success; there are <span class="process-math">\(\binom{n-1}{r-1}\)</span> many ways to choose which of the remaining trials succeed; then the chance that the trials that are supposed to succeed do succeed and the others fail is <span class="process-math">\(p^r (1-p)^{n-r}\text{.}\)</span>)</div>
</div> <div class="para" id="def-neg-binom-1-2">A random variable with this pmf is said to follow a <dfn class="terminology">negative binomial</dfn> distribution with parameters <span class="process-math">\(r\)</span> and <span class="process-math">\(p\text{;}\)</span> we write <span class="process-math">\(X \sim \NegBinom(r,p)\text{.}\)</span>
</div></article></section><section class="subsection" id="subsec-variance-correlation"><h3 class="heading hide-type">
<span class="type">Subsection</span><span class="space"> </span><span class="codenumber">2.2.4</span><span class="space"> </span><span class="title">Correlation and variance</span>
</h3>
<div class="para" id="subsec-variance-correlation-2">Recall that <span class="process-math">\(\var(X)\)</span> is defined to be <span class="process-math">\(E((X-E(X))^2)\)</span> and is equal to <span class="process-math">\(E(X^2) - E(X)^2\text{.}\)</span> We should not expect this statistic to be linear, since as we will verify we expect <span class="process-math">\(\var(aX) = a^2 \var(X)\text{.}\)</span> But it is natural to wonder under what conditions it is <em class="emphasis">additive</em>, meaning <span class="process-math">\(\var(X+Y) = \var(X) + \var(Y)\text{.}\)</span>
</div>
<article class="definition definition-like" id="def-uncorrelated"><h4 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">2.2.24</span><span class="period">.</span>
</h4>
<div class="para" id="def-uncorrelated-1-1">Two random variables <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\)</span> are <dfn class="terminology">uncorrelated</dfn> if <span class="process-math">\(E(XY) = E(X)E(Y)\text{.}\)</span>
</div></article><article class="lemma theorem-like" id="lem-independent-uncorrelated"><h4 class="heading">
<span class="type">Lemma</span><span class="space"> </span><span class="codenumber">2.2.25</span><span class="period">.</span>
</h4>
<div class="para" id="lem-independent-uncorrelated-1-1">If <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\)</span> are independent, then they are uncorrelated.</div></article><details id="lem-independent-uncorrelated-2" class="hiddenproof born-hidden-knowl"><summary><h4 class="heading"><span class="type">Proof<span class="period">.</span></span></h4></summary><article class="hiddenproof"><div class="para logical" id="lem-independent-uncorrelated-2-1"><div class="displaymath process-math" id="lem-independent-uncorrelated-2-1-1">
\begin{align*}
E(XY) \amp = \sum_{x,y} xy P(X = x \text{ and } Y = y) \\
\amp = \sum_{x,y} xy P(X = x) P(Y = y) \\
\amp = \sum_{x} x P(X = x) \sum_y y P(Y = y) \\
\amp = E(X) E(Y) 
\end{align*}
</div></div></article></details><div class="para" id="subsec-variance-correlation-5">The converse is false!</div>
<article class="example example-like" id="subsec-variance-correlation-6"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">2.2.26</span><span class="period">.</span>
</h4>
<div class="para" id="subsec-variance-correlation-6-1-1">Let <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\)</span> be independent Bernoulli random variables with parameter <span class="process-math">\(1/2\text{.}\)</span> Show that <span class="process-math">\(X+Y\)</span> and <span class="process-math">\(\abs{X-Y}\)</span> are dependent but uncorrelated.</div>
<div class="solutions"><details id="subsec-variance-correlation-6-2" class="solution solution-like born-hidden-knowl"><summary><span class="type">Solution</span><span class="period">.</span></summary><div class="solution solution-like">
<div class="para" id="subsec-variance-correlation-6-2-1">To show that <span class="process-math">\(X+Y\)</span> and <span class="process-math">\(\abs{X-Y}\)</span> are dependent, it is enough to show that the events <span class="process-math">\(\{X+Y = 0\}\)</span> and <span class="process-math">\(\{\abs{X-Y} = 0\}\)</span> are dependent. But of course! The only way for <span class="process-math">\(X+Y\)</span> to equal <span class="process-math">\(0\)</span> is for <span class="process-math">\(X = Y = 0\text{,}\)</span> and this implies that <span class="process-math">\(\abs{X-Y} = 0\text{.}\)</span> In other words, <span class="process-math">\(P(X+Y = 0 \text{ and } \abs{X-Y} = 0) = 
P(X+Y = 0) = \tfrac14\)</span> and <span class="process-math">\(P(\abs{X-Y}=0) = \tfrac12\text{,}\)</span> and <span class="process-math">\(\tfrac14 \cdot \tfrac12 \ne \tfrac14\text{.}\)</span>
</div> <div class="para logical" id="subsec-variance-correlation-6-2-2">
<div class="para">To see that <span class="process-math">\(X+Y\)</span> and <span class="process-math">\(\abs{X-Y}\)</span> are uncorrelated, we compute all the relevant expected values by hand:</div>
<div class="displaymath process-math">
\begin{equation*}
E(X+Y) = E(X) + E(Y) = \tfrac12 + \tfrac12 = 1.
\end{equation*}
</div>
<div class="displaymath process-math">
\begin{equation*}
E(\abs{X-Y}) = 0P(\abs{X-Y}=0) + 1P(\abs{X-Y} = 1) = \tfrac12
\end{equation*}
</div>
<div class="displaymath process-math">
\begin{equation*}
E(\abs{X-Y}(X+Y)) = 0P(Z = 0) + 1P(Z = 1) + 2P(Z = 2)
= 1P(\abs{X-Y}(X+Y)=1) = \tfrac12
\end{equation*}
</div>
<div class="para">And <span class="process-math">\(\tfrac12 \cdot 1 = \tfrac12\text{,}\)</span> so <span class="process-math">\(X+Y\)</span> and <span class="process-math">\(\abs{X-Y}\)</span> are uncorrelated.</div>
</div>
</div></details></div></article><div class="para" id="subsec-variance-correlation-7">And here is the reason for making the definition:</div>
<article class="proposition theorem-like" id="prop-variance-uncorrelated"><h4 class="heading">
<span class="type">Proposition</span><span class="space"> </span><span class="codenumber">2.2.27</span><span class="period">.</span>
</h4>
<div class="para logical" id="prop-variance-uncorrelated-1-1"><ol class="lower-alpha">
<li id="prop-variance-uncorrelated-1-1-1-1"><div class="para" id="prop-variance-uncorrelated-1-1-1-1-1"><span class="process-math">\(\displaystyle \var(aX) = a^2 \var(X)\)</span></div></li>
<li id="prop-variance-uncorrelated-1-1-1-2"><div class="para" id="prop-variance-uncorrelated-1-1-1-2-1">If <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\)</span> are uncorrelated, then <span class="process-math">\(\var(X+Y) = \var(X) + \var(Y)\text{.}\)</span>
</div></li>
</ol></div></article><details id="prop-variance-uncorrelated-2" class="hiddenproof born-hidden-knowl"><summary><h4 class="heading"><span class="type">Proof<span class="period">.</span></span></h4></summary><article class="hiddenproof"><div class="para logical" id="prop-variance-uncorrelated-2-1">
<div class="para">The first part follows immediately from the definition of variance and the linearity of expectation:</div>
<div class="displaymath process-math">
\begin{equation*}
\var(aX) = E((aX - E(aX)^2))
= E(a^2 (X - E(X))^2)
= a^2 \var(X)
\end{equation*}
</div>
</div> <div class="para logical" id="prop-variance-uncorrelated-2-2">
<div class="para">The second part follows from our other characterization of variance:</div>
<div class="displaymath process-math" id="prop-variance-uncorrelated-2-2-1">
\begin{align*}
\var(X+Y) \amp = E((X+Y)^2) - (E(X+Y))^2 \\
\amp = E(X^2 + 2XY + Y^2) - E(X)^2 - 2E(X)E(Y) - E(Y)^2 \\
\amp = E(X^2) + 2E(XY) + E(Y^2) - E(X)^2 - 2E(X)E(Y) - E(Y)^2 \\
\amp = \var(X) + \var(Y) 
\end{align*}
</div>
<div class="para">That’s it.</div>
</div></article></details><article class="example example-like" id="subsec-variance-correlation-9"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">2.2.28</span><span class="period">.</span>
</h4>
<div class="para" id="subsec-variance-correlation-9-1">A <span class="process-math">\(\binomial(n,p)\)</span> random variable <span class="process-math">\(X\)</span> can be expressed as a sum <span class="process-math">\(X = X_1 + \cdots + X_n\)</span> of independent <span class="process-math">\(\Bernoulli(p)\)</span> random variables. Recall that the variance of a <span class="process-math">\(\Bernoulli(p)\)</span> random variable is <span class="process-math">\(p(1-p)\text{.}\)</span> Since the <span class="process-math">\(X_k\)</span> are independent, they are uncorrelated. So <span class="process-math">\(\var(X) = \sum_{k=1}^n p(1-p) = np(1-p)\text{.}\)</span>
</div></article></section></section></div>
<div class="ptx-content-footer">
<a class="previous-button button" href="sec-random-variables-theory.html" title="Previous"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5cb;</span><span class="name">Prev</span></a><a class="top-button button" href="#" title="Top"><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5ce;</span><span class="name">Top</span></a><a class="next-button button" href="sec-cts-rvs.html" title="Next"><span class="name">Next</span><span class="icon material-symbols-outlined" aria-hidden="true">&#xe5cc;</span></a>
</div></main>
</div>
<div id="ptx-page-footer" class="ptx-page-footer">
<a class="pretext-link" href="https://pretextbook.org" title="PreTeXt"><div class="logo"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="338 3000 8772 6866"><g style="stroke-width:.025in; stroke:black; fill:none"><polyline points="472,3590 472,9732 " style="stroke:#000000;stroke-width:174; stroke-linejoin:miter; stroke-linecap:round; "></polyline><path style="stroke:#000000;stroke-width:126;stroke-linecap:butt;" d="M 4724,9448 A 4660 4660  0  0  1  8598  9259"></path><path style="stroke:#000000;stroke-width:174;stroke-linecap:butt;" d="M 4488,9685 A 4228 4228  0  0  0   472  9732"></path><path style="stroke:#000000;stroke-width:126;stroke-linecap:butt;" d="M 4724,3590 A 4241 4241  0  0  1  8598  3496"></path><path style="stroke:#000000;stroke-width:126;stroke-linecap:round;" d="M 850,3496  A 4241 4241  0  0  1  4724  3590"></path><path style="stroke:#000000;stroke-width:126;stroke-linecap:round;" d="M 850,9259  A 4507 4507  0  0  1  4724  9448"></path><polyline points="5385,4299 4062,8125" style="stroke:#000000;stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="8598,3496 8598,9259" style="stroke:#000000;stroke-width:126; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="850,3496 850,9259" style="stroke:#000000;stroke-width:126; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="4960,9685 4488,9685" style="stroke:#000000;stroke-width:174; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="3070,4582 1889,6141 3070,7700" style="stroke:#000000;stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="6418,4582 7600,6141 6418,7700" style="stroke:#000000;stroke-width:300; stroke-linejoin:miter; stroke-linecap:round;"></polyline><polyline points="8976,3590 8976,9732" style="stroke:#000000;stroke-width:174; stroke-linejoin:miter; stroke-linecap:round;"></polyline><path style="stroke:#000000;stroke-width:174;stroke-linecap:butt;" d="M 4960,9685 A 4228 4228  0  0  1  8976  9732"></path></g></svg></div></a><a class="runestone-link" href="https://runestone.academy" title="Runestone Academy"><img class="logo" src="https://runestone.academy/runestone/static/images/RAIcon_cropped.png"></a><a class="mathjax-link" href="https://www.mathjax.org" title="MathJax"><img class="logo" src="https://www.mathjax.org/badge/badge-square-2.png"></a>
</div>
</body>
</html>
